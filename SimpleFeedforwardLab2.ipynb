{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q0l05Wm4lzwu"
      },
      "outputs": [],
      "source": [
        "#simple feedforward neural network\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def create_data():\n",
        "    X = np.random.randn(1000, 10)\n",
        "    y = np.random.randn(1000, 1)\n",
        "    return X, y\n",
        "\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(50, activation='relu', input_shape=(10,)),\n",
        "        layers.Dense(20, activation='relu'),\n",
        "        layers.Dense(1)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def train_model_with_history(model, optimizer, X, y, batch_size, epochs, optimizer_name):\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    history = []\n",
        "    for epoch in range(epochs):\n",
        "        hist = model.fit(X, y, batch_size=batch_size, epochs=1, verbose=0)\n",
        "        loss = hist.history['loss'][0]\n",
        "        history.append(loss)\n",
        "        print(f'Epoch {epoch + 1}/{epochs} - {optimizer_name} loss: {loss:.4f}')\n",
        "    return history\n",
        "\n",
        "X, y = create_data()\n",
        "\n",
        "model_sgd = create_model()\n",
        "model_adam = create_model()\n",
        "\n",
        "optimizer_sgd = optimizers.SGD(learning_rate=0.01)\n",
        "optimizer_adam = optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "\n",
        "print('\\nTraining with SGD optimizer: ')\n",
        "sgd_loss = train_model_with_history(model_sgd, optimizer_sgd, X, y, batch_size, epochs, 'SGD')\n",
        "\n",
        "print('\\nTraining with Adam optimizer: ')\n",
        "adam_loss = train_model_with_history(model_adam, optimizer_adam, X, y, batch_size, epochs, 'Adam')\n",
        "\n",
        "plt.plot(range(1, epochs + 1), sgd_loss, label='SGD', color='blue')\n",
        "plt.plot(range(1, epochs + 1), adam_loss, label='Adam', color='red')\n",
        "plt.title('SGD vs Adam Optimizer: Loss Comparison')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, optimizers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#  Added StandardScaler for feature normalization\n",
        "def create_data():\n",
        "    X = np.random.randn(1000, 10)\n",
        "    #  Introduced a nonlinear relationship in the target variable\n",
        "    y = 3 * X[:, 0] + 2 * X[:, 1] ** 2 - 1 + np.random.randn(1000) * 0.5\n",
        "    y = y.reshape(-1, 1)\n",
        "    scaler = StandardScaler()  # Normalize features for better convergence\n",
        "    X = scaler.fit_transform(X)\n",
        "    return X, y\n",
        "\n",
        "#  Added dropout layers to prevent overfitting\n",
        "def create_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(50, activation='relu', input_shape=(10,)),  # Input layer\n",
        "        layers.Dropout(0.3),  # Dropout to reduce overfitting\n",
        "        layers.Dense(20, activation='relu'),  # Hidden layer\n",
        "        layers.Dense(1)  # Output layer for regression (linear activation)\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "#  Added EarlyStopping for efficient training\n",
        "def train_model_with_history(model, optimizer, X, y, batch_size, epochs, optimizer_name):\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])  # Added MAE metric\n",
        "    early_stopping = EarlyStopping(monitor='loss', patience=5)  # Stops training when no improvement\n",
        "    hist = model.fit(X, y, batch_size=batch_size, epochs=epochs, verbose=0, callbacks=[early_stopping])  # Early stopping added\n",
        "    history = hist.history['loss']\n",
        "    print(f'{optimizer_name} Final Loss: {history[-1]:.4f}')\n",
        "    return history\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = create_data()\n",
        "\n",
        "optimizers_dict = {\n",
        "    'SGD': optimizers.SGD(learning_rate=0.01),  # SGD with a fixed learning rate\n",
        "    'Adam': optimizers.Adam(learning_rate=0.001)  # Adam optimizer\n",
        "}\n",
        "\n",
        "loss_histories = {}\n",
        "\n",
        "#  Used a loop to train models with multiple optimizers\n",
        "for opt_name, opt in optimizers_dict.items():\n",
        "    print(f'\\nTraining with {opt_name} optimizer:')\n",
        "    model = create_model()\n",
        "    loss_histories[opt_name] = train_model_with_history(model, opt, X, y, batch_size=32, epochs=50, optimizer_name=opt_name)\n",
        "\n",
        "#  Improved visualization with dynamic legend and grid\n",
        "for opt_name, losses in loss_histories.items():\n",
        "    plt.plot(range(1, len(losses) + 1), losses, label=opt_name)\n",
        "\n",
        "plt.title('Optimizer Comparison: Loss')  # Clearer plot title\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)  # Added grid for better readability\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zoObplEll0Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.random.randn(1000, 10)\n",
        "    #  Introduced a nonlinear relationship in the target variable\n",
        "\n",
        "y = 3 * X[:, 0] + 2 * X[:, 1] ** 2 - 1 + np.random.randn(1000) * 0.5"
      ],
      "metadata": {
        "id": "KXWxf_dGmXko"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16ERjxH2mYo8",
        "outputId": "feb67d3f-f45f-49dc-9ab5-3c9e92b33d2d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.55318203 -1.17564016  0.34905788 ... -1.11989673  0.82496835\n",
            "   0.76992952]\n",
            " [-0.3541367   0.96138145 -0.18311613 ...  0.04135527 -0.299439\n",
            "   0.02306111]\n",
            " [-1.54857467  0.28783793  0.62621348 ...  1.541354    0.21907812\n",
            "  -0.50053508]\n",
            " ...\n",
            " [-1.93639327 -0.14542759 -0.17533974 ...  0.57383093  0.90403226\n",
            "  -1.0927911 ]\n",
            " [ 0.12266546 -0.42848588 -0.04219025 ...  0.83626778 -0.55812459\n",
            "   0.08504679]\n",
            " [-0.24145728 -0.9759213  -0.1931836  ...  0.06781572  0.65987559\n",
            "   0.95659744]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPPacTELmaLY",
        "outputId": "9a5918fe-3972-42da-b5f5-fed4fe910b6e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = y.reshape(-1, 1)"
      ],
      "metadata": {
        "id": "wl5k6cpkmbGP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wFpLpDjmq5a",
        "outputId": "567bd494-c71f-458f-8b4c-a808d4879673"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5vqDDIRemrcp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}